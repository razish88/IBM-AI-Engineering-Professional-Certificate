{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<a href=\"https://cognitiveclass.ai\"><img src = \"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/Logos/organization_logo/organization_logo.png\" width = 400> </a>\n",
    "\n",
    "<h1 align=center><font size = 5>Pre-Trained Models</font></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Introduction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "In this lab, you will learn how to leverage pre-trained models to build image classifiers instead of building a model from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Table of Contents\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n",
    "\n",
    "<font size = 3> \n",
    "    \n",
    "1. <a href=\"#item31\">Import Libraries and Packages</a>\n",
    "2. <a href=\"#item32\">Download Data</a>  \n",
    "3. <a href=\"#item33\">Define Global Constants</a>  \n",
    "4. <a href=\"#item34\">Construct ImageDataGenerator Instances</a>  \n",
    "5. <a href=\"#item35\">Compile and Fit Model</a>\n",
    "\n",
    "</font>\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<a id='item31'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Import Libraries and Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Let's start the lab by importing the libraries that we will be using in this lab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "First, we will import the ImageDataGenerator module since we will be leveraging it to train our model in batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "In this lab, we will be using the Keras library to build an image classifier, so let's download the Keras library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Finally, we will be leveraging the ResNet50 model to build our classifier, so let's download it as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "from keras.applications import ResNet50\n",
    "from keras.applications.resnet50 import preprocess_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<a id='item32'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Download Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "For your convenience, I have placed the data on a server which you can retrieve easily using the **wget** command. So let's run the following line of code to get the data. Given the large size of the image dataset, it might take some time depending on your internet speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-06-29 15:40:50--  https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/concrete_data_week3.zip\n",
      "Resolving s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)... 67.228.254.196\n",
      "Connecting to s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)|67.228.254.196|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 261482368 (249M) [application/zip]\n",
      "Saving to: ‘concrete_data_week3.zip’\n",
      "\n",
      "concrete_data_week3 100%[===================>] 249.37M   761KB/s    in 2m 7s   \n",
      "\n",
      "2020-06-29 15:42:58 (1.96 MB/s) - ‘concrete_data_week3.zip’ saved [261482368/261482368]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## get the data\n",
    "!wget https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/concrete_data_week3.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "And now if you check the left directory pane, you should see the zipped file *concrete_data_week3.zip* appear. So, let's go ahead and unzip the file to access the images. Given the large number of images in the dataset, this might take a couple of minutes, so please be patient, and wait until the code finishes running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!unzip -q concrete_data_week3.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Now, you should see the folder *concrete_data_week3* appear in the left pane. If you open this folder by double-clicking on it, you will find that it contains two folders: *train* and *valid*. And if you explore these folders, you will find that each contains two subfolders: *positive* and *negative*. These are the same folders that we saw in the labs in the previous modules of this course, where *negative* is the negative class and it represents the concrete images with no cracks and *positive* is the positive class and it represents the concrete images with cracks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "**Important Note**: There are thousands and thousands of images in each folder, so please don't attempt to double click on the *negative* and *positive* folders. This may consume all of your memory and you may end up with a **50*** error. So please **DO NOT DO IT**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<a id='item33'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Define Global Constants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Here, we will define constants that we will be using throughout the rest of the lab. \n",
    "\n",
    "1. We are obviously dealing with two classes, so *num_classes* is 2. \n",
    "2. The ResNet50 model was built and trained using images of size (224 x 224). Therefore, we will have to resize our images from (227 x 227) to (224 x 224).\n",
    "3. We will training and validating the model using batches of 100 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "num_classes = 2\n",
    "\n",
    "image_resize = 224\n",
    "\n",
    "batch_size_training = 100\n",
    "batch_size_validation = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<a id='item34'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Construct ImageDataGenerator Instances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "In order to instantiate an ImageDataGenerator instance, we will set the **preprocessing_function** argument to *preprocess_input* which we imported from **keras.applications.resnet50** in order to preprocess our images the same way the images used to train ResNet50 model were processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "data_generator = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Next, we will use the *flow_from_directory* method to get the training images as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 30001 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = data_generator.flow_from_directory(\n",
    "    'concrete_data_week3/train',\n",
    "    target_size=(image_resize, image_resize),\n",
    "    batch_size=batch_size_training,\n",
    "    class_mode='categorical')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "**Your Turn**: Use the *flow_from_directory* method to get the validation images and assign the result to **validation_generator**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10001 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "## Type your answer here\n",
    "validation_generator = data_generator.flow_from_directory(\n",
    "    \"concrete_data_week3/valid\",\n",
    "    target_size=(image_resize, image_resize),\n",
    "    batch_size=batch_size_validation,\n",
    "    class_mode=\"categorical\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Double-click __here__ for the solution.\n",
    "<!-- The correct answer is:\n",
    "validation_generator = data_generator.flow_from_directory(\n",
    "    'concrete_data_week3/valid',\n",
    "    target_size=(image_resize, image_resize),\n",
    "    batch_size=batch_size_validation,\n",
    "    class_mode='categorical')\n",
    "-->\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<a id='item35'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Build, Compile and Fit Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "In this section, we will start building our model. We will use the Sequential model class from Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Next, we will add the ResNet50 pre-trained model to out model. However, note that we don't want to include the top layer or the output layer of the pre-trained model. We actually want to define our own output layer and train it so that it is optimized for our image dataset. In order to leave out the output layer of the pre-trained model, we will use the argument *include_top* and set it to **False**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.2/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "94658560/94653016 [==============================] - 93s 1us/step\n"
     ]
    }
   ],
   "source": [
    "model.add(ResNet50(\n",
    "    include_top=False,\n",
    "    pooling='avg',\n",
    "    weights='imagenet',\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Then, we will define our output layer as a **Dense** layer, that consists of two nodes and uses the **Softmax** function as the activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "You can access the model's layers using the *layers* attribute of our model object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.engine.training.Model at 0x7fae570dd710>,\n",
       " <keras.layers.core.Dense at 0x7fae69634790>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "You can see that our model is composed of two sets of layers. The first set is the layers pertaining to ResNet50 and the second set is a single layer, which is our Dense layer that we defined above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "You can access the ResNet50 layers by running the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.engine.input_layer.InputLayer at 0x7fae76c5ba10>,\n",
       " <keras.layers.convolutional.ZeroPadding2D at 0x7fae76c5b810>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fae76c5bd50>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fae7ad26250>,\n",
       " <keras.layers.core.Activation at 0x7fae76c5bf10>,\n",
       " <keras.layers.convolutional.ZeroPadding2D at 0x7fae7b9a4050>,\n",
       " <keras.layers.pooling.MaxPooling2D at 0x7fae7b9a4390>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fae76c5a550>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fae7c1fb950>,\n",
       " <keras.layers.core.Activation at 0x7fae7c1fb910>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fae7c1fbf90>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fae7c36af10>,\n",
       " <keras.layers.core.Activation at 0x7fae7c3e0e90>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fae7c3fd2d0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fae7c54ec10>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fae7c54ee50>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fae7c5faad0>,\n",
       " <keras.layers.merge.Add at 0x7fae7c188f10>,\n",
       " <keras.layers.core.Activation at 0x7fae7c63eb10>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fae7c6d9990>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fae7c744bd0>,\n",
       " <keras.layers.core.Activation at 0x7fae7c760710>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fae7c7c9f10>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fae7c835dd0>,\n",
       " <keras.layers.core.Activation at 0x7fae7c84f3d0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fae7c8358d0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fae7cb1eb90>,\n",
       " <keras.layers.merge.Add at 0x7fae7cb98f50>,\n",
       " <keras.layers.core.Activation at 0x7fae7cb1eb50>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fae7cbb3650>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fae7cc0f610>,\n",
       " <keras.layers.core.Activation at 0x7fae7cc0ff10>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fae7cc24c50>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fae7ccf6b90>,\n",
       " <keras.layers.core.Activation at 0x7fae7ccf6d90>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fae7ce8ce90>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fae7cea2c90>,\n",
       " <keras.layers.merge.Add at 0x7fae7cf19350>,\n",
       " <keras.layers.core.Activation at 0x7fae7ceff790>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fae7cf75dd0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fae7cfe5950>,\n",
       " <keras.layers.core.Activation at 0x7fae7cffed10>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fae7d020f90>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fae7d0d18d0>,\n",
       " <keras.layers.core.Activation at 0x7fae7d0d1b10>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fae7d0ef6d0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fae7d5d74d0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fae7d5bfb50>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fae7d669b50>,\n",
       " <keras.layers.merge.Add at 0x7fae7d6e2150>,\n",
       " <keras.layers.core.Activation at 0x7fae7d6c9610>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fae7d9429d0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fae7d9b65d0>,\n",
       " <keras.layers.core.Activation at 0x7fae7da2d9d0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fae7d9d0d90>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fae7daa0dd0>,\n",
       " <keras.layers.core.Activation at 0x7fae7db16e50>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fae7daa0710>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fae7db88e50>,\n",
       " <keras.layers.merge.Add at 0x7fae7db88950>,\n",
       " <keras.layers.core.Activation at 0x7fae7dba8e50>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fae7dc278d0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fae7dc8dd10>,\n",
       " <keras.layers.core.Activation at 0x7fae7dce9e90>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fae7dc8de50>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fae7de7af50>,\n",
       " <keras.layers.core.Activation at 0x7fae7def5b90>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fae7de7acd0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fae7e068d90>,\n",
       " <keras.layers.merge.Add at 0x7fae7e0dfe90>,\n",
       " <keras.layers.core.Activation at 0x7fae7e068dd0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fae7e0fe910>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fae7e151a90>,\n",
       " <keras.layers.core.Activation at 0x7fae7e151c50>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fae7e1700d0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fae7e243990>,\n",
       " <keras.layers.core.Activation at 0x7fae7e243910>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fae7e259610>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fae5ec82590>,\n",
       " <keras.layers.merge.Add at 0x7fae5ecbbe50>,\n",
       " <keras.layers.core.Activation at 0x7fae5ec9df10>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fae5ec9dfd0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fae5ed6ed90>,\n",
       " <keras.layers.core.Activation at 0x7fae5eda5e50>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fae5ee03210>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fae5ee5ae50>,\n",
       " <keras.layers.core.Activation at 0x7fae5ee5af10>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fae5eef8cd0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fae600e2b90>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fae600cd910>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fae60174f10>,\n",
       " <keras.layers.merge.Add at 0x7fae6015bfd0>,\n",
       " <keras.layers.core.Activation at 0x7fae601ca310>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fae60247d50>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fae602bb750>,\n",
       " <keras.layers.core.Activation at 0x7fae60337f10>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fae602d2a90>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fae603a9710>,\n",
       " <keras.layers.core.Activation at 0x7fae603c2b90>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fae603a9d90>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fae60490a50>,\n",
       " <keras.layers.merge.Add at 0x7fae60490f10>,\n",
       " <keras.layers.core.Activation at 0x7fae604aa410>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fae6052edd0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fae60599250>,\n",
       " <keras.layers.core.Activation at 0x7fae60599e50>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fae60712e50>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fae607809d0>,\n",
       " <keras.layers.core.Activation at 0x7fae607fdf10>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fae60780a90>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fae60972f50>,\n",
       " <keras.layers.merge.Add at 0x7fae6098ab50>,\n",
       " <keras.layers.core.Activation at 0x7fae60972890>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fae60a09bd0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fae60a73bd0>,\n",
       " <keras.layers.core.Activation at 0x7fae60a8d610>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fae60af5f10>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fae60b8ad90>,\n",
       " <keras.layers.core.Activation at 0x7fae60ba6390>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fae60b8a890>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fae62d70b90>,\n",
       " <keras.layers.merge.Add at 0x7fae62f06f50>,\n",
       " <keras.layers.core.Activation at 0x7fae62d70b50>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fae62f1f650>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fae62f80610>,\n",
       " <keras.layers.core.Activation at 0x7fae62f80f10>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fae63011dd0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fae630b2ed0>,\n",
       " <keras.layers.core.Activation at 0x7fae630b2b90>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fae630c7d50>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fae6315acd0>,\n",
       " <keras.layers.merge.Add at 0x7fae631d1f10>,\n",
       " <keras.layers.core.Activation at 0x7fae631b57d0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fae6322ea90>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fae6329e950>,\n",
       " <keras.layers.core.Activation at 0x7fae632b6d10>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fae6340ff90>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fae634bf990>,\n",
       " <keras.layers.core.Activation at 0x7fae634bf390>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fae634ddd10>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fae635aeb50>,\n",
       " <keras.layers.merge.Add at 0x7fae635aed50>,\n",
       " <keras.layers.core.Activation at 0x7fae635c4e90>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fae63645d90>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fae636b6610>,\n",
       " <keras.layers.core.Activation at 0x7fae6372dd10>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fae636d2dd0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fae6379fd90>,\n",
       " <keras.layers.core.Activation at 0x7fae63817e90>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fae6379f6d0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fae67a89a90>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fae67a89950>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fae67b33b50>,\n",
       " <keras.layers.merge.Add at 0x7fae67ba9a10>,\n",
       " <keras.layers.core.Activation at 0x7fae67b76ad0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fae67e11dd0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fae67e7bfd0>,\n",
       " <keras.layers.core.Activation at 0x7fae67ed9e10>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fae67e7b450>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fae67f65f50>,\n",
       " <keras.layers.core.Activation at 0x7fae67fe3b90>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fae67f65990>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fae6815a650>,\n",
       " <keras.layers.merge.Add at 0x7fae681d0e10>,\n",
       " <keras.layers.core.Activation at 0x7fae6815ad90>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fae681ef850>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fae68242a90>,\n",
       " <keras.layers.core.Activation at 0x7fae68276f10>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fae68261ad0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fae68333990>,\n",
       " <keras.layers.core.Activation at 0x7fae68333910>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fae68348610>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fae57032f50>,\n",
       " <keras.layers.merge.Add at 0x7fae570aba50>,\n",
       " <keras.layers.core.Activation at 0x7fae57032350>,\n",
       " <keras.layers.pooling.GlobalAveragePooling2D at 0x7fae6815a110>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[0].layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Since the ResNet50 model has already been trained, then we want to tell our model not to bother with training the ResNet part, but to train only our dense output layer. To do that, we run the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "model.layers[0].trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "And now using the *summary* attribute of the model, we can see how many parameters we will need to optimize in order to train the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "resnet50 (Model)             (None, 2048)              23587712  \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 4098      \n",
      "=================================================================\n",
      "Total params: 23,591,810\n",
      "Trainable params: 4,098\n",
      "Non-trainable params: 23,587,712\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Next we compile our model using the **adam** optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Before we are able to start the training process, with an ImageDataGenerator, we will need to define how many steps compose an epoch. Typically, that is the number of images divided by the batch size. Therefore, we define our steps per epoch as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "steps_per_epoch_training = len(train_generator)\n",
    "steps_per_epoch_validation = len(validation_generator)\n",
    "num_epochs = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Finally, we are ready to start training our model. Unlike a conventional deep learning training were data is not streamed from a directory, with an ImageDataGenerator where data is augmented in batches, we use the **fit_generator** method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "301/301 [==============================] - 14958s 50s/step - loss: 0.0668 - accuracy: 0.9781 - val_loss: 0.0109 - val_accuracy: 0.9384\n",
      "Epoch 2/2\n",
      "301/301 [==============================] - 13987s 46s/step - loss: 0.0159 - accuracy: 0.9966 - val_loss: 0.0013 - val_accuracy: 0.9486\n"
     ]
    }
   ],
   "source": [
    "fit_history = model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=steps_per_epoch_training,\n",
    "    epochs=num_epochs,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=steps_per_epoch_validation,\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Now that the model is trained, you are ready to start using it to classify images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Since training can take a long time when building deep learning models, it is always a good idea to save your model once the training is complete if you believe you will be using the model again later. You will be using this model in the next module, so go ahead and save your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-da1cab8ae03a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'classifier_resnet_model.h5'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude_optimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.save('classifier_resnet_model.h5', include_optimizer=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Now, you should see the model file *classifier_resnet_model.h5* apprear in the left directory pane."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Thank you for completing this lab!\n",
    "\n",
    "This notebook was created by Alex Aklson. I hope you found this lab interesting and educational."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "This notebook is part of a course on **Coursera** called *AI Capstone Project with Deep Learning*. If you accessed this notebook outside the course, you can take this course online by clicking [here](https://cocl.us/DL0321EN_Coursera_Week3_LAB1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<hr>\n",
    "\n",
    "Copyright &copy; 2020 [IBM Developer Skills Network](https://cognitiveclass.ai/?utm_source=bducopyrightlink&utm_medium=dswb&utm_campaign=bdu). This notebook and its source code are released under the terms of the [MIT License](https://bigdatauniversity.com/mit-license/)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
